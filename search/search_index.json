{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CAPITOL.ai Overview Welcome to CAPITOL.ai! This document provides an overview of the three main ways you can integrate and use our services. Sections LLM API User API NPM Library Usage Options 1. LLM API CAPITOL.ai's LLM API allows you to create and manage your own stories with full control. When you create a story using the API, you will receive a WebSocket address, enabling you to stream the results in real-time. Key Features: Full Control: Manage your stories independently. WebSocket Streaming: Stream results directly from the provided WebSocket address. Flexible Actions: Perform any action on any story you own by tracking the response IDs. Learn more about the LLM API 2. User API The User API is designed to facilitate requests on behalf of a user or an organization. This API allows you to create stories, segment them by user or organization, and later query to retrieve these stories. Key Features: User Segmentation: Easily segment stories and reports for different users. ACL Layer: All requests are proxied through our LLM API with an additional Access Control Layer for user-specific management. Story Management: Create and retrieve stories tailored to your users' needs. Learn more about the User API 3. NPM Library CAPITOL.ai offers a fully integrated, turnkey solution with our NPM library. This allows you to build reports on your website quickly and easily. The library makes it simple to create shareable reports that are editable only by the story's author. Key Features: Turnkey Solution: Quickly integrate and start building reports on your website. Report Sharing: Share reports with others while maintaining control over edits. Author-Only Editing: Ensure that only the story author can make changes. Learn more about the NPM Library","title":"CAPITOL.ai Overview"},{"location":"#capitolai-overview","text":"Welcome to CAPITOL.ai! This document provides an overview of the three main ways you can integrate and use our services.","title":"CAPITOL.ai Overview"},{"location":"#sections","text":"LLM API User API NPM Library","title":"Sections"},{"location":"#usage-options","text":"","title":"Usage Options"},{"location":"#1-llm-api","text":"CAPITOL.ai's LLM API allows you to create and manage your own stories with full control. When you create a story using the API, you will receive a WebSocket address, enabling you to stream the results in real-time. Key Features: Full Control: Manage your stories independently. WebSocket Streaming: Stream results directly from the provided WebSocket address. Flexible Actions: Perform any action on any story you own by tracking the response IDs. Learn more about the LLM API","title":"1. LLM API"},{"location":"#2-user-api","text":"The User API is designed to facilitate requests on behalf of a user or an organization. This API allows you to create stories, segment them by user or organization, and later query to retrieve these stories. Key Features: User Segmentation: Easily segment stories and reports for different users. ACL Layer: All requests are proxied through our LLM API with an additional Access Control Layer for user-specific management. Story Management: Create and retrieve stories tailored to your users' needs. Learn more about the User API","title":"2. User API"},{"location":"#3-npm-library","text":"CAPITOL.ai offers a fully integrated, turnkey solution with our NPM library. This allows you to build reports on your website quickly and easily. The library makes it simple to create shareable reports that are editable only by the story's author. Key Features: Turnkey Solution: Quickly integrate and start building reports on your website. Report Sharing: Share reports with others while maintaining control over edits. Author-Only Editing: Ensure that only the story author can make changes. Learn more about the NPM Library","title":"3. NPM Library"},{"location":"llm_api/","text":"LLM API Overview The CAPITOL.ai LLM API provides powerful tools for creating and managing stories. You can: Initiate Report Generation: Create reports with custom configurations. Remix Existing Content: Modify existing stories with new inputs. Cancel Processes: Stop ongoing content generation tasks. Convert Content: Transform stories into different formats. Manage Events: Create, retrieve, and reorder story-related events. For a complete list of endpoints and detailed documentation, visit the Swagger API Documentation . LLM API Endpoints 1. /llm Method: POST Summary: Initiate Report Generation Description: Starts the process of generating a report using the LLM based on the given parameters. Request Body: params : General parameters for the report. llm_params : Specific LLM configuration settings. Responses: 200 : Report generation initiated successfully. 422 : Validation error. 2. /remix Method: POST Summary: Remix Content Description: Allows remixing of an existing story with new parameters and returns the result immediately. Request Body: external_id : ID of the story to remix. user_prompt : Remix instructions. story_plan : Current story plan. Responses: 200 : Remix operation completed successfully. 422 : Validation error. 3. /cancel Method: POST Summary: Cancel Content Generation Description: Cancels an ongoing content generation process. Request Body: external_id : ID of the content generation process to cancel. Responses: 200 : Cancelation successful. 422 : Validation error. 4. /turn_into Method: POST Summary: Convert Content Description: Converts existing content into another form or type. Request Body: external_id : ID of the content to convert. draft_id : ID of the draft being edited. output_type : Desired output type. Responses: 200 : Conversion successful. 422 : Validation error. 5. /events Method: GET Summary: Retrieve Events Description: Fetches events related to a specific external ID. Parameters: external_id : Query parameter for the ID to retrieve events. Responses: 200 : Events retrieved successfully. 422 : Validation error. Method: POST Summary: Create Event Description: Creates a new event related to content generation. Request Body: Event creation parameters. Responses: 200 : Event creation successful. 422 : Validation error. 6. /reorder/{external_id} Method: PUT Summary: Reorder Events Description: Reorders events associated with a specific external ID. Request Body: events : List of event IDs in the desired order. Responses: 200 : Events reordered successfully. 422 : Validation error.","title":"Llm api"},{"location":"llm_api/#llm-api-overview","text":"The CAPITOL.ai LLM API provides powerful tools for creating and managing stories. You can: Initiate Report Generation: Create reports with custom configurations. Remix Existing Content: Modify existing stories with new inputs. Cancel Processes: Stop ongoing content generation tasks. Convert Content: Transform stories into different formats. Manage Events: Create, retrieve, and reorder story-related events. For a complete list of endpoints and detailed documentation, visit the Swagger API Documentation .","title":"LLM API Overview"},{"location":"llm_api/#llm-api-endpoints","text":"","title":"LLM API Endpoints"},{"location":"llm_api/#1-llm","text":"Method: POST Summary: Initiate Report Generation Description: Starts the process of generating a report using the LLM based on the given parameters. Request Body: params : General parameters for the report. llm_params : Specific LLM configuration settings. Responses: 200 : Report generation initiated successfully. 422 : Validation error.","title":"1. /llm"},{"location":"llm_api/#2-remix","text":"Method: POST Summary: Remix Content Description: Allows remixing of an existing story with new parameters and returns the result immediately. Request Body: external_id : ID of the story to remix. user_prompt : Remix instructions. story_plan : Current story plan. Responses: 200 : Remix operation completed successfully. 422 : Validation error.","title":"2. /remix"},{"location":"llm_api/#3-cancel","text":"Method: POST Summary: Cancel Content Generation Description: Cancels an ongoing content generation process. Request Body: external_id : ID of the content generation process to cancel. Responses: 200 : Cancelation successful. 422 : Validation error.","title":"3. /cancel"},{"location":"llm_api/#4-turn_into","text":"Method: POST Summary: Convert Content Description: Converts existing content into another form or type. Request Body: external_id : ID of the content to convert. draft_id : ID of the draft being edited. output_type : Desired output type. Responses: 200 : Conversion successful. 422 : Validation error.","title":"4. /turn_into"},{"location":"llm_api/#5-events","text":"Method: GET Summary: Retrieve Events Description: Fetches events related to a specific external ID. Parameters: external_id : Query parameter for the ID to retrieve events. Responses: 200 : Events retrieved successfully. 422 : Validation error. Method: POST Summary: Create Event Description: Creates a new event related to content generation. Request Body: Event creation parameters. Responses: 200 : Event creation successful. 422 : Validation error.","title":"5. /events"},{"location":"llm_api/#6-reorderexternal_id","text":"Method: PUT Summary: Reorder Events Description: Reorders events associated with a specific external ID. Request Body: events : List of event IDs in the desired order. Responses: 200 : Events reordered successfully. 422 : Validation error.","title":"6. /reorder/{external_id}"},{"location":"npm/","text":"NPM components For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"NPM components"},{"location":"npm/#npm-components","text":"For full documentation visit mkdocs.org .","title":"NPM components"},{"location":"npm/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"npm/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"user_api/","text":"Capitol LLM API Integration Overview The Capitol LLM API is designed to offer a fully managed user-based store, enabling seamless integration with AI-driven applications. This API supports making CORS requests and is structured around the x_api_key authentication mechanism. All requests will initially hit the user's API, which is responsible for managing the interactions. Workflow Overview Your components will communicate with the client server via an endpoint that the client provides to the component. This endpoint should call: /api/latest /api-integration The request must include the x_api_key in the headers. The client server will then handle the request, return a response in JSON format, and pass that back to the component. Key Points: CORS Requests: Handled directly by client-side components with appropriate headers. Security: Requests are authenticated using an x_api_key , ensuring secure communication. JSON Response: After the client's API processes the request, the component will receive the necessary JSON data. For more details, visit the Capitol LLM Swagger Documentation . For more information about JavaScript parameters and how to integrate this into your code, please visit the NPM section .","title":"Capitol LLM API Integration Overview"},{"location":"user_api/#capitol-llm-api-integration-overview","text":"The Capitol LLM API is designed to offer a fully managed user-based store, enabling seamless integration with AI-driven applications. This API supports making CORS requests and is structured around the x_api_key authentication mechanism. All requests will initially hit the user's API, which is responsible for managing the interactions.","title":"Capitol LLM API Integration Overview"},{"location":"user_api/#workflow-overview","text":"Your components will communicate with the client server via an endpoint that the client provides to the component. This endpoint should call: /api/latest /api-integration The request must include the x_api_key in the headers. The client server will then handle the request, return a response in JSON format, and pass that back to the component.","title":"Workflow Overview"},{"location":"user_api/#key-points","text":"CORS Requests: Handled directly by client-side components with appropriate headers. Security: Requests are authenticated using an x_api_key , ensuring secure communication. JSON Response: After the client's API processes the request, the component will receive the necessary JSON data. For more details, visit the Capitol LLM Swagger Documentation . For more information about JavaScript parameters and how to integrate this into your code, please visit the NPM section .","title":"Key Points:"}]}